{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: Sunday, December 17 ([CET](https://www.timeanddate.com/time/zones/cet))\n",
    "* Reviews: Dies Natalis Solis Invicti ([CET](https://en.wikipedia.org/wiki/Sol_Invictus))\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple, defaultdict\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Tic Tac Toe environment\n",
    "class TicTacToe:\n",
    "    def __init__(self):\n",
    "        self.board = [' ' for _ in range(9)]  # Representing the Tic Tac Toe board\n",
    "        self.current_player = 'X'  # Player 'X' starts the game\n",
    "        self.winning_combinations = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns\n",
    "            [0, 4, 8], [2, 4, 6]             # Diagonals\n",
    "        ]\n",
    "    \n",
    "    def print_board(self):\n",
    "        for i in range(0, 9, 3):\n",
    "            print(f\"{self.board[i]} | {self.board[i+1]} | {self.board[i+2]}\")\n",
    "        print('---------')\n",
    "    \n",
    "    def available_moves(self, board = None):\n",
    "        if board is None:\n",
    "            return [i for i, val in enumerate(self.board) if val == ' ']\n",
    "        else:\n",
    "            return [i for i, val in enumerate(board) if val == ' ']\n",
    "\n",
    "    \n",
    "    def make_move(self, position):\n",
    "        self.board[position] = self.current_player\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
    "    \n",
    "    def check_winner(self):\n",
    "        for combo in self.winning_combinations:\n",
    "            if (self.board[combo[0]] == self.board[combo[1]] == self.board[combo[2]]) and (self.board[combo[0]] != ' '):\n",
    "                return self.board[combo[0]]\n",
    "        return None\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.check_winner() or ' ' not in self.board\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = [' ' for _ in range(9)]\n",
    "        self.current_player = 'X'\n",
    "\n",
    "# Q-Learning agent to play Tic Tac Toe\n",
    "class QLearningAgent:\n",
    "    def __init__(self, epsilon, alpha=0.5, gamma=0.1):\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.q_table = defaultdict(float)  # Q-table to store state-action values\n",
    "        self.env = None\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        old_value = self.get_q_value(state, action)\n",
    "        l = [self.get_q_value(next_state, a) for a in self.env.available_moves(next_state)]\n",
    "        if len(l) > 0:\n",
    "            best_next_action = max([self.get_q_value(next_state, a) for a in self.env.available_moves(next_state)])\n",
    "        else:\n",
    "            best_next_action = 0\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * best_next_action)\n",
    "        self.q_table[(state, action)] = new_value\n",
    "    \n",
    "    def choose_action(self, state, available_moves,play_as = None, playing = False):\n",
    "        if not playing: #training\n",
    "            if random.uniform(0, 1) < self.epsilon.get():\n",
    "                return random.choice(available_moves)\n",
    "            else:\n",
    "                return max(available_moves, key=lambda a: self.get_q_value(state, a))\n",
    "        else:\n",
    "            if play_as == 'X':\n",
    "                return max(available_moves, key=lambda a: self.get_q_value(state, a))\n",
    "            else:\n",
    "                return min(available_moves, key=lambda a: self.get_q_value(state, a))\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def choose_action(self, state, available_moves, play_as = None, playing = False):\n",
    "        return random.choice(available_moves)\n",
    "\n",
    "\n",
    "class EpsilonScheduler():\n",
    "    def __init__(self, low, high, num_round):\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        self.num_round = num_round\n",
    "        self.step = (high - low) / num_round\n",
    "\n",
    "        self.counter = 0\n",
    "\n",
    "    def get(self):\n",
    "        return_val = self.high - self.counter * self.step\n",
    "        self.counter += 1\n",
    "        return return_val\n",
    "\n",
    "\n",
    "# Training the agent\n",
    "def train_agent(episodes):\n",
    "    epsilon = EpsilonScheduler(low = 0.1, high = 1, num_round = episodes)\n",
    "    agent = QLearningAgent(epsilon= epsilon)\n",
    "    env = TicTacToe()\n",
    "    agent.env = env\n",
    "    \n",
    "    for episode in tqdm(range(episodes)):\n",
    "        env.reset()\n",
    "        state = tuple(env.board)\n",
    "        \n",
    "        while not env.game_over():\n",
    "            available_moves = env.available_moves()\n",
    "            action = agent.choose_action(state, available_moves)\n",
    "            env.make_move(action)\n",
    "            next_state = tuple(env.board)\n",
    "            \n",
    "            if env.check_winner() == 'X':\n",
    "                reward = 1\n",
    "\n",
    "\n",
    "            elif env.check_winner() == 'O':\n",
    "                reward = -1\n",
    "\n",
    "            else:\n",
    "                reward = 0\n",
    "            \n",
    "            agent.update_q_value(state, action, reward, next_state)\n",
    "            state = next_state\n",
    "    \n",
    "    return agent\n",
    "\n",
    "\n",
    "# Playing against the trained agent\n",
    "def play_vs_agent(agent):\n",
    "    env = TicTacToe()\n",
    "    state = tuple(env.board)\n",
    "    \n",
    "    while not env.game_over():\n",
    "        if env.current_player == 'X':\n",
    "            env.print_board()\n",
    "            print(\"Your turn! Choose a position (0-8):\")\n",
    "            while True:\n",
    "                try:\n",
    "                    user_move = int(input())\n",
    "                    if user_move in env.available_moves():\n",
    "                        break\n",
    "                    else:\n",
    "                        print(\"Invalid move! Choose an available position.\")\n",
    "                except ValueError:\n",
    "                    print(\"Invalid input! Enter a number.\")\n",
    "            env.make_move(user_move)\n",
    "        else:\n",
    "            available_moves = env.available_moves()\n",
    "            action = agent.choose_action(state, available_moves, play_as = 'O', playing = True)\n",
    "            env.make_move(action)\n",
    "        \n",
    "        state = tuple(env.board)\n",
    "    \n",
    "    env.print_board()\n",
    "    winner = env.check_winner()\n",
    "    if winner:\n",
    "        print(f\"{winner} wins!\")\n",
    "    else:\n",
    "        print(\"It's a tie!\")\n",
    "\n",
    "def agent_vs_agent(agentX, agentO):\n",
    "    env = TicTacToe()\n",
    "    state = tuple(env.board)\n",
    "    \n",
    "    while not env.game_over():\n",
    "        if env.current_player == 'X':\n",
    "            available_moves = env.available_moves()\n",
    "            action = agentX.choose_action(state, available_moves, play_as = 'X', playing = True)\n",
    "            env.make_move(action)\n",
    "        else:\n",
    "            available_moves = env.available_moves()\n",
    "            action = agentO.choose_action(state, available_moves, play_as = 'O', playing = True)\n",
    "            env.make_move(action)\n",
    "        \n",
    "        state = tuple(env.board)\n",
    "    \n",
    "    return env.check_winner()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train agent\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae7733e7da04db09816f66b14fc6ccf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('train agent')\n",
    "trained_agent = train_agent(episodes=500_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "play vs agent\n",
      "  |   |  \n",
      "  |   |  \n",
      "  |   |  \n",
      "---------\n",
      "Your turn! Choose a position (0-8):\n",
      "X | O |  \n",
      "  |   |  \n",
      "  |   |  \n",
      "---------\n",
      "Your turn! Choose a position (0-8):\n",
      "Invalid move! Choose an available position.\n",
      "X | O |  \n",
      "  | X |  \n",
      "  |   | O\n",
      "---------\n",
      "Your turn! Choose a position (0-8):\n",
      "Invalid move! Choose an available position.\n",
      "Invalid move! Choose an available position.\n",
      "X | O |  \n",
      "O | X |  \n",
      "  | X | O\n",
      "---------\n",
      "Your turn! Choose a position (0-8):\n",
      "X | O | X\n",
      "O | X |  \n",
      "O | X | O\n",
      "---------\n",
      "Your turn! Choose a position (0-8):\n",
      "X | O | X\n",
      "O | X | X\n",
      "O | X | O\n",
      "---------\n",
      "It's a tie!\n"
     ]
    }
   ],
   "source": [
    "print('play vs agent')\n",
    "play_vs_agent(trained_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0f94bc68ebb496283724fbdf73922cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winning rate: 0.550882\n"
     ]
    }
   ],
   "source": [
    "agentX = RandomAgent()\n",
    "agentO = trained_agent\n",
    "num_rounds = 500_000\n",
    "win = 0\n",
    "for round in tqdm(range(num_rounds)):\n",
    "\n",
    "    result = agent_vs_agent(agentX, agentO)\n",
    "\n",
    "    if result == 'O':\n",
    "        win += 1\n",
    "\n",
    "print(f'winning rate: {win / num_rounds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eb03305f66e4e719567b6a0809789dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "winning rate: 0.960852\n"
     ]
    }
   ],
   "source": [
    "agentO= RandomAgent()\n",
    "agentX = trained_agent\n",
    "num_rounds = 500_000\n",
    "win = 0\n",
    "for round in tqdm(range(num_rounds)):\n",
    "\n",
    "    result = agent_vs_agent(agentX, agentO)\n",
    "\n",
    "    if result == 'X':\n",
    "        win += 1\n",
    "\n",
    "print(f'winning rate: {win / num_rounds}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
