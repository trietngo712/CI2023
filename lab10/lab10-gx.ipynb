{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice\n",
    "from copy import deepcopy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "State = namedtuple('State', ['x', 'o'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 7, 6, 9, 5, 1, 4, 3, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_game():\n",
    "    trajectory = list()\n",
    "    state = State(set(), set())\n",
    "    available = set(range(1, 9+1))\n",
    "    while available:\n",
    "        x = choice(list(available))\n",
    "        state.x.add(x)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(x)\n",
    "        if win(state.x) or not available:\n",
    "            break\n",
    "\n",
    "        o = choice(list(available))\n",
    "        state.o.add(o)\n",
    "        trajectory.append(deepcopy(state))\n",
    "        available.remove(o)\n",
    "        if win(state.o):\n",
    "            break\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d498dd1aa362440fb767c62e2c167872",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "value_dictionary = defaultdict(float)\n",
    "hit_state = defaultdict(int)\n",
    "epsilon = 0.001\n",
    "\n",
    "for steps in tqdm(range(500_000)):\n",
    "    trajectory = random_game()\n",
    "    final_reward = state_value(trajectory[-1])\n",
    "    for state in trajectory:\n",
    "        hashable_state = (frozenset(state.x), frozenset(state.o))\n",
    "        hit_state[hashable_state] += 1\n",
    "        value_dictionary[hashable_state] = value_dictionary[\n",
    "            hashable_state\n",
    "        ] + epsilon * (final_reward - value_dictionary[hashable_state])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[((frozenset({1, 2, 5, 7, 8}), frozenset({3, 4, 6, 9})), 0.9162768977439065),\n",
       " ((frozenset({1, 3, 4, 7, 8}), frozenset({2, 5, 6, 9})), 0.915435043471609),\n",
       " ((frozenset({1, 2, 3, 4, 9}), frozenset({5, 6, 7, 8})), 0.9138121288629873),\n",
       " ((frozenset({1, 2, 3, 6, 7}), frozenset({4, 5, 8, 9})), 0.9125089093789029),\n",
       " ((frozenset({2, 6, 7, 8, 9}), frozenset({1, 3, 4, 5})), 0.9123336643739864),\n",
       " ((frozenset({1, 2, 4, 5, 6}), frozenset({3, 7, 8, 9})), 0.9121580683526234),\n",
       " ((frozenset({1, 4, 6, 7, 8}), frozenset({2, 3, 5, 9})), 0.9118940146263526),\n",
       " ((frozenset({1, 5, 7, 8, 9}), frozenset({2, 3, 4, 6})), 0.9115407078597959),\n",
       " ((frozenset({4, 5, 6, 8, 9}), frozenset({1, 2, 3, 7})), 0.9112747983417009),\n",
       " ((frozenset({2, 5, 7, 8, 9}), frozenset({1, 3, 4, 6})), 0.9111859843260269)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(value_dictionary.items(), key=lambda e: e[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5477"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hit_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "    def _init_(self):\n",
    "        self.board = [' ' for _ in range(9)]  # Representing the Tic Tac Toe board\n",
    "        self.current_player = 'X'  # Player 'X' starts the game\n",
    "        self.winning_combinations = [\n",
    "            [0, 1, 2], [3, 4, 5], [6, 7, 8],  # Rows\n",
    "            [0, 3, 6], [1, 4, 7], [2, 5, 8],  # Columns\n",
    "            [0, 4, 8], [2, 4, 6]             # Diagonals\n",
    "        ]\n",
    "    \n",
    "    def print_board(self):\n",
    "        for i in range(0, 9, 3):\n",
    "            print(f\"{self.board[i]} | {self.board[i+1]} | {self.board[i+2]}\")\n",
    "        print('---------')\n",
    "    \n",
    "    def available_moves(self):\n",
    "        return [i for i, val in enumerate(self.board) if val == ' ']\n",
    "    \n",
    "    def make_move(self, position):\n",
    "        self.board[position] = self.current_player\n",
    "        self.current_player = 'O' if self.current_player == 'X' else 'X'\n",
    "    \n",
    "    def check_winner(self):\n",
    "        for combo in self.winning_combinations:\n",
    "            if (self.board[combo[0]] == self.board[combo[1]] == self.board[combo[2]]) and (self.board[combo[0]] != ' '):\n",
    "                return self.board[combo[0]]\n",
    "        return None\n",
    "    \n",
    "    def game_over(self):\n",
    "        return self.check_winner() or ' ' not in self.board\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = [' ' for _ in range(9)]\n",
    "        self.current_player = 'X'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def _init_(self, epsilon=0.3, alpha=0.1, gamma=0.9):\n",
    "        self.epsilon = epsilon  # Exploration rate\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.gamma = gamma  # Discount factor\n",
    "        self.q_table = {}  # Q-table to store state-action values\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        return self.q_table.get((state, action), 0.0)\n",
    "    \n",
    "    def update_q_value(self, state, action, reward, next_state):\n",
    "        old_value = self.get_q_value(state, action)\n",
    "        best_next_action = max([self.get_q_value(next_state, a) for a in env.available_moves()])\n",
    "        new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * best_next_action)\n",
    "        self.q_table[(state, action)] = new_value\n",
    "    \n",
    "    def choose_action(self, state, available_moves):\n",
    "        if random.uniform(0, 1) < self.epsilon:\n",
    "            return random.choice(available_moves)\n",
    "        else:\n",
    "            return max(available_moves, key=lambda a: self.get_q_value(state, a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = defaultdict(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d[(1,1)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d[(1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-P-7LqQ3C-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
